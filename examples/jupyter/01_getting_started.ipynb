{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChunkFlow: Getting Started\n",
    "\n",
    "Welcome to ChunkFlow! This notebook will guide you through the basics of text chunking for RAG systems.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to chunk text with different strategies\n",
    "2. How to generate embeddings\n",
    "3. How to evaluate chunking quality\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install chunk-flow[huggingface]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "from chunk_flow.chunking import StrategyRegistry\n",
    "from chunk_flow.embeddings import EmbeddingProviderFactory\n",
    "from chunk_flow.evaluation import EvaluationPipeline\n",
    "\n",
    "print(\"✓ ChunkFlow imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sample Document\n",
    "\n",
    "Let's start with a sample document about machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"\n",
    "# Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers \n",
    "to learn from data without explicit programming. It has revolutionized many \n",
    "industries including healthcare, finance, and transportation.\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "There are three main types of machine learning:\n",
    "\n",
    "1. **Supervised Learning**: The algorithm learns from labeled training data. \n",
    "   Examples include classification and regression tasks.\n",
    "\n",
    "2. **Unsupervised Learning**: The algorithm finds patterns in unlabeled data. \n",
    "   Common techniques include clustering and dimensionality reduction.\n",
    "\n",
    "3. **Reinforcement Learning**: The algorithm learns through trial and error, \n",
    "   receiving rewards or penalties for its actions.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Machine learning powers many modern applications:\n",
    "- Recommendation systems (Netflix, Spotify)\n",
    "- Fraud detection in banking\n",
    "- Image recognition and computer vision\n",
    "- Natural language processing and chatbots\n",
    "- Autonomous vehicles\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(document)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunking Strategies\n",
    "\n",
    "ChunkFlow provides multiple strategies. Let's explore them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fixed-Size Chunking\n",
    "\n",
    "The simplest approach - split text into fixed-size chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fixed-size chunker\n",
    "fixed_chunker = StrategyRegistry.create(\n",
    "    \"fixed_size\",\n",
    "    {\"chunk_size\": 200, \"overlap\": 50}\n",
    ")\n",
    "\n",
    "# Chunk the document\n",
    "fixed_result = await fixed_chunker.chunk(document, doc_id=\"ml_intro\")\n",
    "\n",
    "print(f\"Created {len(fixed_result.chunks)} chunks\")\n",
    "print(f\"Processing time: {fixed_result.processing_time_ms:.2f}ms\\n\")\n",
    "\n",
    "# Show first 3 chunks\n",
    "for i, chunk in enumerate(fixed_result.chunks[:3], 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"{chunk[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Recursive Character Chunking (Recommended)\n",
    "\n",
    "Respects natural boundaries like paragraphs and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recursive chunker\n",
    "recursive_chunker = StrategyRegistry.create(\n",
    "    \"recursive\",\n",
    "    {\n",
    "        \"chunk_size\": 300,\n",
    "        \"overlap\": 50,\n",
    "        \"separators\": [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Chunk the document\n",
    "recursive_result = await recursive_chunker.chunk(document, doc_id=\"ml_intro\")\n",
    "\n",
    "print(f\"Created {len(recursive_result.chunks)} chunks\")\n",
    "print(f\"Processing time: {recursive_result.processing_time_ms:.2f}ms\\n\")\n",
    "\n",
    "# Show first 3 chunks\n",
    "for i, chunk in enumerate(recursive_result.chunks[:3], 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"{chunk[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Markdown-Aware Chunking\n",
    "\n",
    "Preserves document structure by respecting headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create markdown chunker\n",
    "markdown_chunker = StrategyRegistry.create(\n",
    "    \"markdown\",\n",
    "    {\"respect_headers\": True}\n",
    ")\n",
    "\n",
    "# Chunk the document\n",
    "markdown_result = await markdown_chunker.chunk(document, doc_id=\"ml_intro\")\n",
    "\n",
    "print(f\"Created {len(markdown_result.chunks)} chunks\")\n",
    "print(f\"Processing time: {markdown_result.processing_time_ms:.2f}ms\\n\")\n",
    "\n",
    "# Show all chunks (should respect sections)\n",
    "for i, chunk in enumerate(markdown_result.chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk[:200] + (\"...\" if len(chunk) > 200 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings\n",
    "\n",
    "Now let's convert our chunks to embeddings using HuggingFace (local, free)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding provider\n",
    "embedder = EmbeddingProviderFactory.create(\n",
    "    \"huggingface\",\n",
    "    {\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"device\": \"cpu\",\n",
    "        \"normalize\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate embeddings for recursive chunks\n",
    "embedding_result = await embedder.embed_texts(recursive_result.chunks)\n",
    "\n",
    "print(f\"Generated {len(embedding_result.embeddings)} embeddings\")\n",
    "print(f\"Embedding dimensions: {embedding_result.dimensions}\")\n",
    "print(f\"Processing time: {embedding_result.processing_time_ms:.2f}ms\")\n",
    "print(f\"Total tokens: {embedding_result.token_count}\")\n",
    "print(f\"Cost: Free (local)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Chunk Quality\n",
    "\n",
    "Use ChunkFlow's evaluation metrics to assess chunking quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation pipeline with semantic metrics\n",
    "pipeline = EvaluationPipeline(\n",
    "    metrics=[\n",
    "        \"semantic_coherence\",\n",
    "        \"boundary_quality\",\n",
    "        \"chunk_stickiness\",\n",
    "        \"topic_diversity\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate chunks\n",
    "eval_results = await pipeline.evaluate(\n",
    "    chunks=recursive_result.chunks,\n",
    "    embeddings=embedding_result.embeddings,\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\\n\")\n",
    "for metric_name, metric_result in eval_results.items():\n",
    "    print(f\"{metric_name:<25} {metric_result.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Metrics\n",
    "\n",
    "- **Semantic Coherence** (higher is better): How semantically similar content within each chunk is\n",
    "- **Boundary Quality** (higher is better): How well chunks separate distinct topics\n",
    "- **Chunk Stickiness** (lower is better): How much topics bleed across chunk boundaries\n",
    "- **Topic Diversity** (higher is better): How diverse topics are across all chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Similarity Analysis\n",
    "\n",
    "Let's compute similarity between chunks to understand their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings_array = np.array(embedding_result.embeddings)\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "print(\"Similarity Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Find most similar chunk pairs\n",
    "print(\"\\nMost Similar Chunk Pairs:\")\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(i+1, len(similarity_matrix)):\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        if similarity > 0.8:  # High similarity threshold\n",
    "            print(f\"Chunk {i+1} ↔ Chunk {j+1}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query-Based Retrieval\n",
    "\n",
    "Simulate how chunks would be retrieved for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query\n",
    "query = \"What are the different types of machine learning?\"\n",
    "\n",
    "# Embed the query\n",
    "query_emb_result = await embedder.embed_texts([query])\n",
    "query_embedding = query_emb_result.embeddings[0]\n",
    "\n",
    "# Compute similarities\n",
    "query_emb_array = np.array(query_embedding).reshape(1, -1)\n",
    "similarities = cosine_similarity(query_emb_array, embeddings_array)[0]\n",
    "\n",
    "# Get top 3 most relevant chunks\n",
    "top_k = 3\n",
    "top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(f\"Top {top_k} Most Relevant Chunks:\\n\")\n",
    "\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{rank}. Similarity: {similarities[idx]:.4f}\")\n",
    "    print(f\"   {recursive_result.chunks[idx][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ How to use different chunking strategies (Fixed, Recursive, Markdown)\n",
    "✅ How to generate embeddings with HuggingFace\n",
    "✅ How to evaluate chunk quality with 4 metrics\n",
    "✅ How to analyze chunk similarities\n",
    "✅ How to retrieve relevant chunks for a query\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 02**: Compare multiple strategies\n",
    "- **Notebook 03**: Advanced metrics and evaluation\n",
    "- **Notebook 04**: Visualization and analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}