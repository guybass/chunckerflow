{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChunkFlow: Advanced Metrics Deep Dive\n",
    "\n",
    "This notebook provides a comprehensive exploration of all 12 evaluation metrics in ChunkFlow.\n",
    "\n",
    "## Metric Categories\n",
    "\n",
    "ChunkFlow provides 3 categories of metrics:\n",
    "\n",
    "1. **Retrieval Metrics (4)**: NDCG@k, Recall@k, Precision@k, MRR\n",
    "2. **Semantic Metrics (4)**: Coherence, Boundary Quality, Chunk Stickiness, Topic Diversity\n",
    "3. **RAG Quality Metrics (4)**: Context Relevance, Answer Faithfulness, Context Precision, Context Recall\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How each metric works\n",
    "- When to use each metric\n",
    "- How to interpret metric scores\n",
    "- How to create ground truth data for retrieval metrics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install chunk-flow[huggingface]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from chunk_flow.chunking import StrategyRegistry\n",
    "from chunk_flow.embeddings import EmbeddingProviderFactory\n",
    "from chunk_flow.evaluation import EvaluationPipeline, MetricRegistry\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Document and Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document about climate change\n",
    "document = \"\"\"\n",
    "# Climate Change and Its Impact\n",
    "\n",
    "Climate change refers to long-term shifts in global temperatures and weather patterns. \n",
    "While climate change is a natural phenomenon, scientific evidence shows that human \n",
    "activities have been the primary driver since the mid-20th century.\n",
    "\n",
    "## Causes of Climate Change\n",
    "\n",
    "The main cause is the greenhouse effect. Greenhouse gases like carbon dioxide (CO2), \n",
    "methane (CH4), and nitrous oxide (N2O) trap heat in the atmosphere. Human activities \n",
    "such as burning fossil fuels, deforestation, and industrial processes have dramatically \n",
    "increased these gases' concentrations.\n",
    "\n",
    "Burning coal, oil, and gas for electricity and heat is the largest contributor, \n",
    "producing about 35% of global greenhouse gas emissions. Transportation accounts for \n",
    "another 25%, while manufacturing and construction contribute about 20%.\n",
    "\n",
    "## Environmental Impacts\n",
    "\n",
    "Rising temperatures cause glaciers and ice sheets to melt, leading to rising sea levels. \n",
    "The global mean sea level has risen about 8-9 inches since 1880. Ocean warming and \n",
    "acidification threaten marine ecosystems and coral reefs.\n",
    "\n",
    "Extreme weather events are becoming more frequent and severe. These include hurricanes, \n",
    "droughts, floods, and heat waves. Such events cause billions in damage and displace \n",
    "millions of people annually.\n",
    "\n",
    "## Biodiversity Loss\n",
    "\n",
    "Climate change threatens countless species with extinction. Animals and plants that \n",
    "cannot adapt quickly enough to changing conditions face population decline or \n",
    "extinction. Polar bears, for example, depend on sea ice for hunting, which is \n",
    "rapidly disappearing.\n",
    "\n",
    "## Human Health Effects\n",
    "\n",
    "Climate change affects human health in multiple ways. Heat waves cause thousands of \n",
    "deaths annually. Changing disease patterns mean mosquito-borne illnesses like malaria \n",
    "and dengue fever are spreading to new regions.\n",
    "\n",
    "Air quality deteriorates due to increased wildfires and smog. Crop failures and water \n",
    "scarcity threaten food security for millions, particularly in developing nations.\n",
    "\n",
    "## Solutions and Mitigation\n",
    "\n",
    "Addressing climate change requires global cooperation. The Paris Agreement aims to \n",
    "limit global warming to well below 2°C above pre-industrial levels. Achieving this \n",
    "requires drastic reductions in greenhouse gas emissions.\n",
    "\n",
    "Renewable energy sources like solar, wind, and hydroelectric power must replace \n",
    "fossil fuels. Energy efficiency improvements in buildings, transportation, and \n",
    "industry are crucial. Carbon capture and storage technologies show promise.\n",
    "\n",
    "Individual actions matter too. Reducing energy consumption, using public \n",
    "transportation, eating less meat, and supporting sustainable products all help.\n",
    "\"\"\"\n",
    "\n",
    "# Create chunker and chunk the document\n",
    "chunker = StrategyRegistry.create(\n",
    "    \"recursive\",\n",
    "    {\"chunk_size\": 400, \"overlap\": 60}\n",
    ")\n",
    "\n",
    "chunk_result = await chunker.chunk(document, doc_id=\"climate_change\")\n",
    "chunks = chunk_result.chunks\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"\\nFirst 3 chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"{chunk[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embedder = EmbeddingProviderFactory.create(\n",
    "    \"huggingface\",\n",
    "    {\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"normalize\": True}\n",
    ")\n",
    "\n",
    "emb_result = await embedder.embed_texts(chunks)\n",
    "embeddings = emb_result.embeddings\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimensions: {emb_result.dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category 1: Semantic Metrics\n",
    "\n",
    "Semantic metrics evaluate chunking quality without requiring ground truth data.\n",
    "\n",
    "### 1.1 Semantic Coherence\n",
    "\n",
    "**What it measures**: How semantically similar content within each chunk is.\n",
    "\n",
    "**How it works**: Computes average pairwise similarity between sentences/segments within each chunk.\n",
    "\n",
    "**Interpretation**: \n",
    "- Higher scores (closer to 1.0) = chunks contain related, cohesive content\n",
    "- Lower scores = chunks mix unrelated topics\n",
    "\n",
    "**When to use**: Always! This is a fundamental quality metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate semantic coherence\n",
    "pipeline = EvaluationPipeline(metrics=[\"semantic_coherence\"])\n",
    "\n",
    "coherence_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "coherence_score = coherence_result[\"semantic_coherence\"].score\n",
    "print(f\"Semantic Coherence: {coherence_score:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if coherence_score > 0.8:\n",
    "    print(\"  ✓ Excellent - Chunks are highly coherent\")\n",
    "elif coherence_score > 0.6:\n",
    "    print(\"  ✓ Good - Chunks maintain reasonable coherence\")\n",
    "elif coherence_score > 0.4:\n",
    "    print(\"  ⚠ Fair - Some chunks may mix topics\")\n",
    "else:\n",
    "    print(\"  ✗ Poor - Chunks lack coherence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Boundary Quality\n",
    "\n",
    "**What it measures**: How well chunk boundaries separate distinct topics.\n",
    "\n",
    "**How it works**: Compares similarity between adjacent chunks vs. non-adjacent chunks.\n",
    "\n",
    "**Interpretation**:\n",
    "- Higher scores = chunks have distinct, well-separated topics\n",
    "- Lower scores = adjacent chunks are too similar (poor boundaries)\n",
    "\n",
    "**When to use**: When evaluating if boundaries occur at natural topic shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate boundary quality\n",
    "pipeline = EvaluationPipeline(metrics=[\"boundary_quality\"])\n",
    "\n",
    "boundary_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "boundary_score = boundary_result[\"boundary_quality\"].score\n",
    "print(f\"Boundary Quality: {boundary_score:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if boundary_score > 0.7:\n",
    "    print(\"  ✓ Excellent - Boundaries align with topic shifts\")\n",
    "elif boundary_score > 0.5:\n",
    "    print(\"  ✓ Good - Boundaries are reasonably placed\")\n",
    "elif boundary_score > 0.3:\n",
    "    print(\"  ⚠ Fair - Some boundaries split related content\")\n",
    "else:\n",
    "    print(\"  ✗ Poor - Boundaries are poorly placed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chunk Stickiness (MoC - Mismatch of Chunks)\n",
    "\n",
    "**What it measures**: How much topics \"bleed\" across chunk boundaries.\n",
    "\n",
    "**How it works**: Based on research by Zhao et al. (2025), measures topic overlap between adjacent chunks.\n",
    "\n",
    "**Interpretation**:\n",
    "- **Lower scores are better** (inverted metric)\n",
    "- Lower scores = cleaner boundaries, less topic bleeding\n",
    "- Higher scores = topics spread across multiple chunks\n",
    "\n",
    "**When to use**: When you want to ensure chunks are self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate chunk stickiness\n",
    "pipeline = EvaluationPipeline(metrics=[\"chunk_stickiness\"])\n",
    "\n",
    "stickiness_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "stickiness_score = stickiness_result[\"chunk_stickiness\"].score\n",
    "print(f\"Chunk Stickiness: {stickiness_score:.4f} (lower is better)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if stickiness_score < 0.3:\n",
    "    print(\"  ✓ Excellent - Minimal topic bleeding\")\n",
    "elif stickiness_score < 0.5:\n",
    "    print(\"  ✓ Good - Acceptable topic containment\")\n",
    "elif stickiness_score < 0.7:\n",
    "    print(\"  ⚠ Fair - Some topic bleeding across boundaries\")\n",
    "else:\n",
    "    print(\"  ✗ Poor - Significant topic bleeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Topic Diversity\n",
    "\n",
    "**What it measures**: How diverse topics are across all chunks.\n",
    "\n",
    "**How it works**: Measures average dissimilarity between all chunk pairs.\n",
    "\n",
    "**Interpretation**:\n",
    "- Higher scores = chunks cover diverse topics (good for broad documents)\n",
    "- Lower scores = chunks cover similar topics (expected for focused documents)\n",
    "\n",
    "**When to use**: When evaluating coverage of diverse topics in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate topic diversity\n",
    "pipeline = EvaluationPipeline(metrics=[\"topic_diversity\"])\n",
    "\n",
    "diversity_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "diversity_score = diversity_result[\"topic_diversity\"].score\n",
    "print(f\"Topic Diversity: {diversity_score:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if diversity_score > 0.7:\n",
    "    print(\"  ✓ High diversity - Chunks cover many different topics\")\n",
    "elif diversity_score > 0.5:\n",
    "    print(\"  ✓ Moderate diversity - Good topic coverage\")\n",
    "elif diversity_score > 0.3:\n",
    "    print(\"  ⚠ Low diversity - Chunks are similar (may be expected)\")\n",
    "else:\n",
    "    print(\"  ⚠ Very low diversity - Document focuses on single topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category 2: Retrieval Metrics\n",
    "\n",
    "Retrieval metrics evaluate how well chunks perform in a search/retrieval context. **They require ground truth data.**\n",
    "\n",
    "### Creating Ground Truth\n",
    "\n",
    "For demonstration, we'll create a sample query and identify which chunks are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample query\n",
    "query = \"What are the environmental impacts of climate change on oceans and wildlife?\"\n",
    "\n",
    "# Embed the query\n",
    "query_emb_result = await embedder.embed_texts([query])\n",
    "query_embedding = query_emb_result.embeddings[0]\n",
    "\n",
    "# Compute similarities to find relevant chunks\n",
    "query_emb_array = np.array(query_embedding).reshape(1, -1)\n",
    "embeddings_array = np.array(embeddings)\n",
    "similarities = cosine_similarity(query_emb_array, embeddings_array)[0]\n",
    "\n",
    "# Show top chunks\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "top_indices = np.argsort(similarities)[::-1][:5]\n",
    "print(\"Top 5 most relevant chunks:\\n\")\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{rank}. Chunk {idx} (similarity: {similarities[idx]:.4f})\")\n",
    "    print(f\"   {chunks[idx][:100]}...\\n\")\n",
    "\n",
    "# Define ground truth (chunks about environmental impacts)\n",
    "# Typically chunks 2-4 discuss ocean impacts and biodiversity\n",
    "relevant_indices = [i for i, sim in enumerate(similarities) if sim > 0.5]\n",
    "print(f\"\\nGround truth: Chunks {relevant_indices} are considered relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NDCG@k (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "**What it measures**: Ranking quality with position-based discounting.\n",
    "\n",
    "**How it works**: Rewards relevant chunks appearing higher in results, with diminishing returns for lower positions.\n",
    "\n",
    "**Interpretation**:\n",
    "- 1.0 = Perfect ranking\n",
    "- >0.7 = Excellent ranking\n",
    "- <0.5 = Poor ranking\n",
    "\n",
    "**When to use**: When ranking order matters (most retrieval scenarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate NDCG@k\n",
    "pipeline = EvaluationPipeline(metrics=[\"ndcg_at_k\"])\n",
    "\n",
    "# Create ground truth\n",
    "ground_truth = {\n",
    "    \"query_embedding\": query_embedding,\n",
    "    \"relevant_indices\": relevant_indices,\n",
    "    \"k\": 5,\n",
    "}\n",
    "\n",
    "ndcg_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "ndcg_score = ndcg_result[\"ndcg_at_k\"].score\n",
    "print(f\"NDCG@5: {ndcg_score:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if ndcg_score > 0.8:\n",
    "    print(\"  ✓ Excellent - Relevant chunks ranked highly\")\n",
    "elif ndcg_score > 0.6:\n",
    "    print(\"  ✓ Good - Decent ranking of relevant chunks\")\n",
    "elif ndcg_score > 0.4:\n",
    "    print(\"  ⚠ Fair - Some relevant chunks ranked too low\")\n",
    "else:\n",
    "    print(\"  ✗ Poor - Ranking needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Recall@k\n",
    "\n",
    "**What it measures**: What fraction of relevant chunks appear in top k results.\n",
    "\n",
    "**How it works**: Recall@k = (# relevant chunks in top k) / (total # relevant chunks)\n",
    "\n",
    "**Interpretation**:\n",
    "- 1.0 = All relevant chunks retrieved\n",
    "- 0.5 = Half of relevant chunks retrieved\n",
    "\n",
    "**When to use**: When completeness matters (finding all relevant info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Recall@k\n",
    "pipeline = EvaluationPipeline(metrics=[\"recall_at_k\"])\n",
    "\n",
    "recall_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "recall_score = recall_result[\"recall_at_k\"].score\n",
    "print(f\"Recall@5: {recall_score:.4f}\")\n",
    "print(f\"\\nThis means {recall_score*100:.1f}% of relevant chunks were retrieved in top 5 results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Precision@k\n",
    "\n",
    "**What it measures**: What fraction of top k results are actually relevant.\n",
    "\n",
    "**How it works**: Precision@k = (# relevant chunks in top k) / k\n",
    "\n",
    "**Interpretation**:\n",
    "- 1.0 = All top k results are relevant\n",
    "- 0.5 = Half of top k results are relevant\n",
    "\n",
    "**When to use**: When accuracy of top results matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Precision@k\n",
    "pipeline = EvaluationPipeline(metrics=[\"precision_at_k\"])\n",
    "\n",
    "precision_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "precision_score = precision_result[\"precision_at_k\"].score\n",
    "print(f\"Precision@5: {precision_score:.4f}\")\n",
    "print(f\"\\nThis means {precision_score*100:.1f}% of top 5 results are relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 MRR (Mean Reciprocal Rank)\n",
    "\n",
    "**What it measures**: How highly the first relevant chunk is ranked.\n",
    "\n",
    "**How it works**: MRR = 1 / (rank of first relevant chunk)\n",
    "\n",
    "**Interpretation**:\n",
    "- 1.0 = First result is relevant\n",
    "- 0.5 = Second result is first relevant\n",
    "- 0.33 = Third result is first relevant\n",
    "\n",
    "**When to use**: When you care about \"single answer\" scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MRR\n",
    "pipeline = EvaluationPipeline(metrics=[\"mrr\"])\n",
    "\n",
    "mrr_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "mrr_score = mrr_result[\"mrr\"].score\n",
    "first_relevant_rank = int(1 / mrr_score) if mrr_score > 0 else 0\n",
    "print(f\"MRR: {mrr_score:.4f}\")\n",
    "print(f\"\\nThis means the first relevant chunk appears at rank {first_relevant_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category 3: RAG Quality Metrics (RAGAS-inspired)\n",
    "\n",
    "These metrics evaluate chunking quality in the context of RAG (Retrieval-Augmented Generation) systems.\n",
    "\n",
    "### 3.1 Context Relevance\n",
    "\n",
    "**What it measures**: How relevant retrieved chunks are to a query.\n",
    "\n",
    "**How it works**: Average similarity between query and top-k retrieved chunks.\n",
    "\n",
    "**Interpretation**:\n",
    "- Higher scores = chunks are highly relevant to query\n",
    "- Lower scores = retrieved chunks may not answer the query\n",
    "\n",
    "**When to use**: Always in RAG systems - core quality metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate context relevance\n",
    "pipeline = EvaluationPipeline(metrics=[\"context_relevance\"])\n",
    "\n",
    "relevance_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "relevance_score = relevance_result[\"context_relevance\"].score\n",
    "print(f\"Context Relevance: {relevance_score:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if relevance_score > 0.7:\n",
    "    print(\"  ✓ Excellent - Highly relevant context retrieved\")\n",
    "elif relevance_score > 0.5:\n",
    "    print(\"  ✓ Good - Relevant context retrieved\")\n",
    "elif relevance_score > 0.3:\n",
    "    print(\"  ⚠ Fair - Some irrelevant context retrieved\")\n",
    "else:\n",
    "    print(\"  ✗ Poor - Context not relevant to query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Answer Faithfulness\n",
    "\n",
    "**What it measures**: How self-contained and complete each chunk is.\n",
    "\n",
    "**How it works**: Measures semantic coherence within chunks (similar to semantic coherence but RAG-focused).\n",
    "\n",
    "**Interpretation**:\n",
    "- Higher scores = chunks can stand alone as answers\n",
    "- Lower scores = chunks may lack context\n",
    "\n",
    "**When to use**: When chunks need to be self-contained for LLM context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate answer faithfulness\n",
    "pipeline = EvaluationPipeline(metrics=[\"answer_faithfulness\"])\n",
    "\n",
    "faithfulness_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "faithfulness_score = faithfulness_result[\"answer_faithfulness\"].score\n",
    "print(f\"Answer Faithfulness: {faithfulness_score:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if faithfulness_score > 0.7:\n",
    "    print(\"  ✓ Excellent - Chunks are self-contained\")\n",
    "elif faithfulness_score > 0.5:\n",
    "    print(\"  ✓ Good - Chunks have sufficient context\")\n",
    "else:\n",
    "    print(\"  ⚠ Fair - Chunks may lack context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Context Precision\n",
    "\n",
    "**What it measures**: Precision of retrieved context for RAG.\n",
    "\n",
    "**How it works**: Similar to Precision@k but RAG-focused.\n",
    "\n",
    "**Interpretation**:\n",
    "- Higher scores = less noise in retrieved context\n",
    "- Lower scores = irrelevant chunks retrieved\n",
    "\n",
    "**When to use**: When minimizing irrelevant context matters (e.g., cost reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate context precision\n",
    "pipeline = EvaluationPipeline(metrics=[\"context_precision\"])\n",
    "\n",
    "context_precision_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "context_precision_score = context_precision_result[\"context_precision\"].score\n",
    "print(f\"Context Precision: {context_precision_score:.4f}\")\n",
    "print(f\"\\nThis indicates {context_precision_score*100:.1f}% of retrieved context is relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Context Recall\n",
    "\n",
    "**What it measures**: Recall of retrieved context for RAG.\n",
    "\n",
    "**How it works**: Similar to Recall@k but RAG-focused.\n",
    "\n",
    "**Interpretation**:\n",
    "- Higher scores = comprehensive context retrieved\n",
    "- Lower scores = missing relevant information\n",
    "\n",
    "**When to use**: When completeness of retrieved context matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate context recall\n",
    "pipeline = EvaluationPipeline(metrics=[\"context_recall\"])\n",
    "\n",
    "context_recall_result = await pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "context_recall_score = context_recall_result[\"context_recall\"].score\n",
    "print(f\"Context Recall: {context_recall_score:.4f}\")\n",
    "print(f\"\\nThis indicates {context_recall_score*100:.1f}% of relevant context was retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Metrics Summary\n",
    "\n",
    "Let's evaluate all metrics together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all metrics at once\n",
    "all_metrics_pipeline = EvaluationPipeline(\n",
    "    metrics=[\n",
    "        # Semantic\n",
    "        \"semantic_coherence\",\n",
    "        \"boundary_quality\",\n",
    "        \"chunk_stickiness\",\n",
    "        \"topic_diversity\",\n",
    "        # Retrieval\n",
    "        \"ndcg_at_k\",\n",
    "        \"recall_at_k\",\n",
    "        \"precision_at_k\",\n",
    "        \"mrr\",\n",
    "        # RAG Quality\n",
    "        \"context_relevance\",\n",
    "        \"answer_faithfulness\",\n",
    "        \"context_precision\",\n",
    "        \"context_recall\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "all_results = await all_metrics_pipeline.evaluate(\n",
    "    chunks=chunks,\n",
    "    embeddings=embeddings,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL METRICS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nSEMANTIC METRICS (no ground truth needed)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Semantic Coherence:      {all_results['semantic_coherence'].score:.4f} (higher is better)\")\n",
    "print(f\"Boundary Quality:        {all_results['boundary_quality'].score:.4f} (higher is better)\")\n",
    "print(f\"Chunk Stickiness:        {all_results['chunk_stickiness'].score:.4f} (LOWER is better)\")\n",
    "print(f\"Topic Diversity:         {all_results['topic_diversity'].score:.4f} (higher is better)\")\n",
    "\n",
    "print(\"\\nRETRIEVAL METRICS (require ground truth)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"NDCG@k:                  {all_results['ndcg_at_k'].score:.4f} (higher is better)\")\n",
    "print(f\"Recall@k:                {all_results['recall_at_k'].score:.4f} (higher is better)\")\n",
    "print(f\"Precision@k:             {all_results['precision_at_k'].score:.4f} (higher is better)\")\n",
    "print(f\"MRR:                     {all_results['mrr'].score:.4f} (higher is better)\")\n",
    "\n",
    "print(\"\\nRAG QUALITY METRICS (RAGAS-inspired)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Context Relevance:       {all_results['context_relevance'].score:.4f} (higher is better)\")\n",
    "print(f\"Answer Faithfulness:     {all_results['answer_faithfulness'].score:.4f} (higher is better)\")\n",
    "print(f\"Context Precision:       {all_results['context_precision'].score:.4f} (higher is better)\")\n",
    "print(f\"Context Recall:          {all_results['context_recall'].score:.4f} (higher is better)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Selection Guide\n",
    "\n",
    "### Use Semantic Metrics When:\n",
    "- You don't have ground truth query data\n",
    "- You want to evaluate chunking quality independently\n",
    "- You're comparing strategies on general document corpus\n",
    "- **Recommended for initial evaluation**\n",
    "\n",
    "### Use Retrieval Metrics When:\n",
    "- You have queries and known relevant chunks\n",
    "- You're building a search/retrieval system\n",
    "- You want to optimize for ranking quality\n",
    "- **Requires ground truth data**\n",
    "\n",
    "### Use RAG Quality Metrics When:\n",
    "- You're building a RAG system with LLMs\n",
    "- You want to evaluate context quality for generation\n",
    "- You care about self-containment of chunks\n",
    "- **Best for RAG applications**\n",
    "\n",
    "### Recommended Combinations:\n",
    "\n",
    "**General Purpose (no ground truth):**\n",
    "- semantic_coherence\n",
    "- boundary_quality\n",
    "- chunk_stickiness\n",
    "\n",
    "**Search/Retrieval (with ground truth):**\n",
    "- ndcg_at_k\n",
    "- recall_at_k\n",
    "- precision_at_k\n",
    "\n",
    "**RAG Systems (with queries):**\n",
    "- context_relevance\n",
    "- answer_faithfulness\n",
    "- context_precision\n",
    "- context_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ All 12 evaluation metrics in ChunkFlow\n",
    "✅ What each metric measures and how to interpret scores\n",
    "✅ When to use each metric category\n",
    "✅ How to create ground truth data for retrieval metrics\n",
    "✅ How to evaluate chunks with multiple metrics\n",
    "✅ Best practices for metric selection\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 04**: Visualization and advanced analysis\n",
    "- **Notebook 05**: Using the ChunkFlow REST API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
